{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ehb4K6FlH9Pt"
      },
      "outputs": [],
      "source": [
        "# install and import libraries\n",
        "\n",
        "%%capture\n",
        "%pip install pettingzoo[classic]\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential, clone_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense\n",
        "from keras.regularizers import l2\n",
        "from keras import Input\n",
        "\n",
        "from pettingzoo.classic import tictactoe_v3\n",
        "from gymnasium.spaces.utils import flatten_space\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CscmLMVmWcAf",
        "outputId": "c9d6c065-c4df-4ebd-e014-5dbe0fefbcfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive for persistent storage\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PRy2TpTGho8"
      },
      "outputs": [],
      "source": [
        "# define deep-q learning agent\n",
        "# following tutorials from following repositories:\n",
        "#   https://github.com/keon/deep-q-learning/tree/master\n",
        "#   https://github.com/Alexander-H-Liu/Deep-Q-Learning-Keras/tree/master\n",
        "\n",
        "class DeepQAgent:\n",
        "    def __init__(self, name, state_size, action_size):\n",
        "        # hyperparameters: modify these for training\n",
        "        self.gamma          = 0.9   # reward discount rate\n",
        "        self.epsilon        = 1.0   # exploration probability\n",
        "        self.epsilon_decay  = 0.99  # decay rate for exploration prob\n",
        "        self.epsilon_min    = 0.1   # minimum exploration prob\n",
        "        self.learning_rate  = 0.01  # learning rate\n",
        "        self.clipnorm       = 1.0   # gradient clipping norm\n",
        "        self.memory_size    = 200   # size of experience replay buffer\n",
        "\n",
        "        # don't modify these for training\n",
        "        self.name = name\n",
        "        self.train_mode = True\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=self.memory_size)\n",
        "        self.memory_buffer = None\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = clone_model(self.model)\n",
        "\n",
        "\n",
        "    # model architecture: modify this before training\n",
        "    def _build_model(self):\n",
        "        opt = Adam(\n",
        "            learning_rate=self.learning_rate,\n",
        "            clipnorm=self.clipnorm  # helps prevent exploding gradients problem\n",
        "        )\n",
        "        model = Sequential()\n",
        "        model.add(Input(shape=(self.state_size,)))\n",
        "        model.add(Dense(\n",
        "            32,\n",
        "            use_bias=True,\n",
        "            activation='tanh',\n",
        "            kernel_initializer='he_normal',\n",
        "            bias_initializer='he_normal'\n",
        "        ))\n",
        "        model.add(Dense(\n",
        "            16,\n",
        "            use_bias=True,\n",
        "            activation='tanh',\n",
        "            kernel_initializer='he_normal',\n",
        "            bias_initializer='he_normal'\n",
        "        ))\n",
        "        model.add(Dense(\n",
        "            self.action_size,\n",
        "            use_bias=True,\n",
        "            activation='tanh',\n",
        "            kernel_initializer='he_normal',\n",
        "            bias_initializer='he_normal'\n",
        "        ))\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "\n",
        "    # algorithm-defined function\n",
        "    # choose next action according to deep q-learning algorithm\n",
        "    def act(self, state, mask, verbose=0):\n",
        "        if np.random.rand() <= self.epsilon and self.train_mode: # explore only if in training mode\n",
        "            act_values = np.random.rand(self.action_size)\n",
        "        else:\n",
        "            act_values = self.model.predict(state, verbose=verbose)[0]\n",
        "        if verbose:\n",
        "            for j in range(self.action_size):\n",
        "                print(f'Q(S,{j}) = [ {act_values[j]:6.3f} ]\\t{\"\" if mask[j] else \"(illegal)\"}')\n",
        "            print()\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "\n",
        "    # algorithm-defined function\n",
        "    # compute target q-values over random minibatch and perform network optimization\n",
        "    def replay(self, batch_size, verbose=0):\n",
        "        x_train = np.empty([batch_size, self.state_size])\n",
        "        y_train = np.empty([batch_size, self.action_size])\n",
        "        i = 0\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state, verbose=verbose)[0])) # use fixed model for stabilized training\n",
        "\n",
        "            q_current = self.model.predict(state, verbose=verbose)\n",
        "            q_target = q_current.copy()\n",
        "            q_target[0][action] = target\n",
        "\n",
        "            if verbose >= 2:\n",
        "                print(f'State: {state[0]}\\tAction: {action}\\tReward: {reward}')\n",
        "                for j in range(self.action_size):\n",
        "                    print(f'\\tQ(S,{j}):\\t[ {q_current[0][j]:6.3f} -> {q_target[0][j]:6.3f} ] {\"**\" if (j == action) else \"\"}')\n",
        "                print()\n",
        "\n",
        "            x_train[i] = state\n",
        "            y_train[i] = q_target\n",
        "            i += 1\n",
        "\n",
        "        self.model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=verbose)\n",
        "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "\n",
        "    # algorithm-defined function\n",
        "    # append experience tuple to memory buffer\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def set_train_mode(self, setting):\n",
        "        self.train_mode = setting\n",
        "\n",
        "    def set_epsilon(self, e):\n",
        "        self.epsilon = e\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def load(self, filename):\n",
        "        self.model.load_weights(filename)\n",
        "\n",
        "    def save(self, filename):\n",
        "        self.model.save_weights(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "TC6wnaNnHfHj",
        "outputId": "2eda0771-793d-43c6-a25e-e107a847021b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m153\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">153</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,289\u001b[0m (5.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,289</span> (5.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,289\u001b[0m (5.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,289</span> (5.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ],
      "source": [
        "# create new agent object\n",
        "\n",
        "env = tictactoe_v3.env()\n",
        "env.reset(seed=42)\n",
        "\n",
        "state_size = flatten_space(env.observation_space(env.agents[0])['observation']).shape[0]\n",
        "action_size = env.action_space(env.agents[0]).n\n",
        "\n",
        "agent = DeepQAgent('dqn_agent', state_size, action_size)\n",
        "\n",
        "agent.model.summary()\n",
        "\n",
        "# load pretrained weights from existing filepath\n",
        "pretrained_filepath = None\n",
        "if pretrained_filepath:\n",
        "    agent.load(pretrained_filepath)\n",
        "    agent.set_epsilon(agent.epsilon_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1CNmWAhP7L-",
        "outputId": "007adf0b-cc28-408b-9e6a-3677d083c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0/1\n"
          ]
        }
      ],
      "source": [
        "# train agent over specified number of episodes\n",
        "\n",
        "# training loop hyperparameters\n",
        "BATCH_SIZE = 16\n",
        "TARGET_UPDATE_EPISODES = 1\n",
        "EPISODES = 1\n",
        "agent.set_train_mode(True)\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    print(f\"Episode: {e}/{EPISODES}\")\n",
        "\n",
        "    # reset environment and agent memory buffer\n",
        "    env = tictactoe_v3.env()\n",
        "    env.reset(seed=42)\n",
        "    agent.memory_buffer = {agent_id: None for agent_id in env.agents}\n",
        "\n",
        "    # update stable target model according to hyperparameter\n",
        "    if e % TARGET_UPDATE_EPISODES == 0:\n",
        "        agent.update_target_model()\n",
        "\n",
        "    # this loop encapsulates a single game of tic-tac-toe\n",
        "    for agent_id in env.agent_iter():\n",
        "\n",
        "        # agent observes pre-turn environment\n",
        "        observation, reward, done, truncation, info = env.last()\n",
        "        state = observation['observation'].flatten()\n",
        "        mask = observation['action_mask'].flatten()\n",
        "        state = state.reshape(-1, state.shape[0])\n",
        "\n",
        "        # agent completes experience tuple and appends it to memory deque\n",
        "        if agent.memory_buffer[agent_id] is not None:\n",
        "            last_state, last_action = agent.memory_buffer[agent_id]\n",
        "            agent.memorize(last_state, last_action, reward, state, done)\n",
        "\n",
        "        # agent chooses action according to DQN model\n",
        "        if done or truncation:\n",
        "            action = None\n",
        "        else:\n",
        "            action = agent.act(state, mask)\n",
        "\n",
        "        # agent stores current state and chosen action in memory buffer\n",
        "        # cannot memorize complete experience tuple right now since agent cannot learn its reward and next state until after opponent turn\n",
        "        agent.memory_buffer[agent_id] = (state, action)\n",
        "\n",
        "        # agent executes action\n",
        "        env.step(action)\n",
        "\n",
        "        # learn from sampled experience replay at end of each game\n",
        "        if len(agent.memory) > BATCH_SIZE:\n",
        "            agent.replay(BATCH_SIZE)\n",
        "\n",
        "# save model weights to persistent .weights.h5 file after training\n",
        "agent.save(f'content/drive/MyDrive/{agent.name}.weights.h5')\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyGXEi07r8s4",
        "outputId": "3a98804b-c5d9-4e81-a48d-eae8dc256122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   |   |  \n",
            "-----------\n",
            "   |   |  \n",
            "-----------\n",
            "   |   |  \n",
            "\n",
            "Player 1 sees Q-Values:\n",
            "Q(S,0) = [  0.795 ]\t\n",
            "Q(S,1) = [  0.695 ]\t\n",
            "Q(S,2) = [  0.858 ]\t\n",
            "Q(S,3) = [  0.804 ]\t\n",
            "Q(S,4) = [  0.635 ]\t\n",
            "Q(S,5) = [  0.762 ]\t\n",
            "Q(S,6) = [  0.929 ]\t\n",
            "Q(S,7) = [  0.969 ]\t\n",
            "Q(S,8) = [  0.794 ]\t\n",
            "\n",
            "Enter action: 7\n",
            "\n",
            "   |   |  \n",
            "-----------\n",
            "   |   | X\n",
            "-----------\n",
            "   |   |  \n",
            "\n",
            "Player 2 sees Q-Values:\n",
            "Q(S,0) = [  0.850 ]\t\n",
            "Q(S,1) = [  0.832 ]\t\n",
            "Q(S,2) = [  0.831 ]\t\n",
            "Q(S,3) = [  0.660 ]\t\n",
            "Q(S,4) = [  0.590 ]\t\n",
            "Q(S,5) = [  0.889 ]\t\n",
            "Q(S,6) = [  0.936 ]\t\n",
            "Q(S,7) = [ -0.209 ]\t(illegal)\n",
            "Q(S,8) = [  0.920 ]\t\n",
            "\n",
            "Enter action: 6\n",
            "\n",
            "   |   | O\n",
            "-----------\n",
            "   |   | X\n",
            "-----------\n",
            "   |   |  \n",
            "\n",
            "Player 1 sees Q-Values:\n",
            "Q(S,0) = [ -0.656 ]\t\n",
            "Q(S,1) = [  0.675 ]\t\n",
            "Q(S,2) = [  0.084 ]\t\n",
            "Q(S,3) = [  0.122 ]\t\n",
            "Q(S,4) = [  0.535 ]\t\n",
            "Q(S,5) = [ -0.210 ]\t\n",
            "Q(S,6) = [ -0.932 ]\t(illegal)\n",
            "Q(S,7) = [ -0.956 ]\t(illegal)\n",
            "Q(S,8) = [  0.526 ]\t\n",
            "\n",
            "Enter action: 1\n",
            "\n",
            "   |   | O\n",
            "-----------\n",
            " X |   | X\n",
            "-----------\n",
            "   |   |  \n",
            "\n",
            "Player 2 sees Q-Values:\n",
            "Q(S,0) = [  0.185 ]\t\n",
            "Q(S,1) = [ -0.819 ]\t(illegal)\n",
            "Q(S,2) = [  0.362 ]\t\n",
            "Q(S,3) = [ -0.712 ]\t\n",
            "Q(S,4) = [  0.402 ]\t\n",
            "Q(S,5) = [  0.567 ]\t\n",
            "Q(S,6) = [  0.433 ]\t(illegal)\n",
            "Q(S,7) = [ -0.950 ]\t(illegal)\n",
            "Q(S,8) = [  0.735 ]\t\n",
            "\n",
            "Enter action: 8\n",
            "\n",
            "   |   | O\n",
            "-----------\n",
            " X |   | X\n",
            "-----------\n",
            "   |   | O\n",
            "\n",
            "Player 1 sees Q-Values:\n",
            "Q(S,0) = [ -0.677 ]\t\n",
            "Q(S,1) = [ -0.442 ]\t(illegal)\n",
            "Q(S,2) = [ -0.324 ]\t\n",
            "Q(S,3) = [ -0.519 ]\t\n",
            "Q(S,4) = [  0.042 ]\t\n",
            "Q(S,5) = [ -0.415 ]\t\n",
            "Q(S,6) = [ -0.958 ]\t(illegal)\n",
            "Q(S,7) = [ -0.955 ]\t(illegal)\n",
            "Q(S,8) = [ -0.824 ]\t(illegal)\n",
            "\n",
            "Enter action: 4\n",
            "\n",
            "   |   | O\n",
            "-----------\n",
            " X | X | X\n",
            "-----------\n",
            "   |   | O\n",
            "\n",
            "\n",
            "   |   | O\n",
            "-----------\n",
            " X | X | X\n",
            "-----------\n",
            "   |   | O\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# play against agent in competitive play\n",
        "\n",
        "# utility function for rendering tic-tac-toe grid in terminal\n",
        "def print_grid(grid_vector, agent_id):\n",
        "    chars = [\" \"] * 9\n",
        "    for i in range(9):\n",
        "        if grid_vector[i*2] == 1:\n",
        "            if agent_id == \"player_1\":\n",
        "                chars[i] = \"X\"\n",
        "            else:\n",
        "                chars[i] = \"O\"\n",
        "        elif grid_vector[i*2 + 1] == 1:\n",
        "            if agent_id == \"player_1\":\n",
        "                chars[i] = \"O\"\n",
        "            else:\n",
        "                chars[i] = \"X\"\n",
        "    print()\n",
        "    print(f' {chars[0]} | {chars[3]} | {chars[6]}')\n",
        "    print(f'{\"-\" * 11}')\n",
        "    print(f' {chars[1]} | {chars[4]} | {chars[7]}')\n",
        "    print(f'{\"-\" * 11}')\n",
        "    print(f' {chars[2]} | {chars[5]} | {chars[8]}')\n",
        "    print()\n",
        "\n",
        "agent.set_train_mode(False)\n",
        "env = tictactoe_v3.env()\n",
        "env.reset(seed=42)\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, termination, truncation, info = env.last()\n",
        "    state = observation['observation'].flatten()\n",
        "    mask = observation['action_mask']\n",
        "\n",
        "    print_grid(state, agent_id)\n",
        "\n",
        "    state = state.reshape(-1, state.shape[0])\n",
        "\n",
        "    if termination or truncation:\n",
        "        action = None\n",
        "    else:\n",
        "        print(f'Player {agent_id[-1]} sees Q-Values:')\n",
        "        agent.act(state, mask, verbose=True)\n",
        "        action = np.int64(input(\"Enter action: \"))\n",
        "\n",
        "    env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for presentation\n",
        "\n",
        "state, action, reward, next_state, done = agent.memory[-1]\n",
        "\n",
        "print(state)\n",
        "print(action)\n",
        "print(reward)\n",
        "print(next_state)\n",
        "print(done)\n",
        "\n",
        "mask = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "agent.act(state, mask, verbose=True)\n",
        "\n",
        "\n",
        "agent.act(next_state, mask, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90O9btCZeZYe",
        "outputId": "0c020920-1de2-4811-aa50-c18c02733b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0]]\n",
            "4\n",
            "1\n",
            "[[0 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0]]\n",
            "True\n",
            "Q(S,0) = [ -0.788 ]\t\n",
            "Q(S,1) = [ -0.944 ]\t\n",
            "Q(S,2) = [ -0.995 ]\t\n",
            "Q(S,3) = [ -0.761 ]\t\n",
            "Q(S,4) = [  0.077 ]\t\n",
            "Q(S,5) = [ -0.997 ]\t\n",
            "Q(S,6) = [ -0.986 ]\t\n",
            "Q(S,7) = [ -0.990 ]\t\n",
            "Q(S,8) = [ -0.931 ]\t\n",
            "\n",
            "Q(S,0) = [ -0.708 ]\t\n",
            "Q(S,1) = [ -0.796 ]\t\n",
            "Q(S,2) = [ -0.996 ]\t\n",
            "Q(S,3) = [ -0.830 ]\t\n",
            "Q(S,4) = [ -0.980 ]\t\n",
            "Q(S,5) = [ -0.999 ]\t\n",
            "Q(S,6) = [ -0.994 ]\t\n",
            "Q(S,7) = [ -0.994 ]\t\n",
            "Q(S,8) = [ -0.963 ]\t\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xKAqm7aBSZW"
      },
      "outputs": [],
      "source": [
        "# print the weights of each layer\n",
        "\n",
        "for layer in agent.model.layers:\n",
        "    print(f\"Layer: {layer.name}\")\n",
        "    weights = layer.get_weights()\n",
        "    for i, w in enumerate(weights):\n",
        "      print(f\"Weights array {i} shape: {w.shape}\")\n",
        "      print(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_g1T0wA26J3"
      },
      "outputs": [],
      "source": [
        "# model one\n",
        "\n",
        "'''\n",
        "opt = Adam(\n",
        "    learning_rate=self.learning_rate,\n",
        "    clipnorm=self.clipnorm  # helps prevent exploding gradients problem\n",
        ")\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(self.state_size,)))\n",
        "model.add(Dense(\n",
        "    32,\n",
        "    use_bias=True,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.add(Dense(\n",
        "    16,\n",
        "    use_bias=True,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.add(Dense(\n",
        "    self.action_size,\n",
        "    use_bias=True,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.compile(loss='mse', optimizer=opt)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKnHNEBc3V8P"
      },
      "outputs": [],
      "source": [
        "# model two -- no bias in output layer\n",
        "\n",
        "'''\n",
        "opt = Adam(\n",
        "    learning_rate=self.learning_rate,\n",
        "    clipnorm=self.clipnorm  # helps prevent exploding gradients problem\n",
        ")\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(self.state_size,)))\n",
        "model.add(Dense(\n",
        "    32,\n",
        "    use_bias=True,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.add(Dense(\n",
        "    16,\n",
        "    use_bias=True,\n",
        "    activation='True',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.add(Dense(\n",
        "    self.action_size,\n",
        "    use_bias=False,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.compile(loss='mse', optimizer=opt)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDNJCQ-KqvPN"
      },
      "outputs": [],
      "source": [
        "# model three -- single layer\n",
        "\n",
        "'''\n",
        "opt = Adam(\n",
        "    learning_rate=self.learning_rate,\n",
        "    clipnorm=self.clipnorm  # helps prevent exploding gradients problem\n",
        ")\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(self.state_size,)))\n",
        "model.add(Dense(\n",
        "    self.action_size,\n",
        "    use_bias=True,\n",
        "    activation='tanh',\n",
        "    kernel_initializer='he_normal',\n",
        "    bias_initializer='he_normal'\n",
        "))\n",
        "model.compile(loss='mse', optimizer=opt)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}